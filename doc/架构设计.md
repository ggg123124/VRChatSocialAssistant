# VRChat 社交辅助系统 - 架构设计文档

## 1. 项目概述

### 1.1 项目背景
本项目旨在为 VRChat 玩家提供实时社交辅助功能，通过分析语音对话内容，为社交能力较弱的用户提供即时的对话建议和提示。系统采用非侵入式设计，仅在用户头显中展示辅助信息，不修改 VRChat 客户端或影响其他玩家体验。

### 1.2 核心功能
- 实时语音采集与识别
- 目标说话人识别与分离
- 对话内容理解与分析
- 智能社交建议生成
- VR 头显内提示展示
- 好友信息记忆与检索

### 1.3 技术目标
- **低延迟**：端到端延迟控制在 1-2 秒以内
- **高准确率**：语音识别准确率 > 90%，说话人识别准确率 > 85%
- **隐私保护**：所有处理本地化，不上传第三方语音数据
- **合规性**：采用系统级 OpenXR Overlay，不注入 VRChat 客户端

---

## 2. 系统架构

### 2.1 整体架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                         VR 展示层 (OpenXR)                       │
│                  Head-Locked HUD + World-Locked Panel           │
└────────────────────────────┬────────────────────────────────────┘
                             │ 提示文本
┌────────────────────────────┴────────────────────────────────────┐
│                       提示生成策略模块                            │
│           (分档位建议、安全过滤、节流、优先级排序)                 │
└────────────────────────────┬────────────────────────────────────┘
                             │ 结构化建议
┌────────────────────────────┴────────────────────────────────────┐
│                        LLM 推理层                                │
│              (线上 API / 本地模型 + 函数调用)                     │
└───────────┬────────────────┴────────────────┬───────────────────┘
            │ 对话上下文                      │ RAG 检索
┌───────────┴──────────────┐   ┌─────────────┴──────────────────┐
│   会话理解与状态管理      │   │      记忆与检索模块             │
│   (事件抽取、情绪分析、   │   │   (向量库 + 好友档案 +          │
│    对话状态机)            │   │    对话摘要 + 检索增强)         │
└───────────┬──────────────┘   └────────────────────────────────┘
            │ 识别文本 + 元数据
┌───────────┴──────────────────────────────────────────────────────┐
│                      流式 STT 模块                                │
│              (faster-whisper / 线上实时语音 API)                 │
└───────────┬──────────────────────────────────────────────────────┘
            │ 语音片段 + 说话人 ID
┌───────────┴──────────────────────────────────────────────────────┐
│                 目标说话人识别与验证模块                           │
│         (声纹嵌入提取 + 注册库匹配 + 自适应阈值)                   │
└───────────┬──────────────────────────────────────────────────────┘
            │ 语音活动片段
┌───────────┴──────────────────────────────────────────────────────┐
│                    VAD 语音活动检测                               │
│                  (Silero / WebRTC VAD)                           │
└───────────┬──────────────────────────────────────────────────────┘
            │ 音频流
┌───────────┴──────────────────────────────────────────────────────┐
│                     音频采集层                                    │
│       (系统回环采集扬声器输出 + 麦克风输入 + 降噪处理)             │
└──────────────────────────────────────────────────────────────────┘
```

### 2.2 数据流向

1. **音频采集** → 系统回环抓取 VRChat 扬声器输出 + 本地麦克风输入
2. **VAD 检测** → 切分连续语音流为 0.5-1 秒语音片段
3. **说话人识别** → 对每个语音片段提取声纹嵌入，与已注册好友对比
4. **流式 STT** → 通过门控的语音片段进行实时语音转文本
5. **会话理解** → 分析识别文本，提取事件、情绪、话题变化
6. **记忆检索** → 根据当前上下文从向量库检索相关好友信息
7. **LLM 推理** → 结合上下文与记忆生成结构化社交建议
8. **提示生成** → 根据优先级、安全策略格式化建议
9. **VR 展示** → 在头显 Overlay 中显示提示文本

---

## 3. 核心模块设计

### 3.1 音频采集层 (audio_capture)

**职责**：
- 系统级音频回环采集（抓取 VRChat 扬声器输出）
- 麦克风输入采集（用户本人发言）
- 音频预处理（降噪、回声消除、增益控制）
- 设备管理与切换
- 时间戳对齐

**关键技术**：
- Windows: PyAudio / sounddevice + WASAPI 回环
- 噪声抑制: noisereduce / RNNoise
- 采样率: 16kHz（STT 标准）
- 缓冲区: 30ms 帧

**输出**：
- 16kHz 单声道音频流（numpy array）
- 时间戳元数据

---

### 3.2 语音活动检测 (vad)

**职责**：
- 从连续音频流中检测语音活动片段
- 过滤静音与环境噪声
- 语音片段切分与缓冲

**关键技术**：
- Silero VAD（PyTorch 模型，低延迟）
- 或 WebRTC VAD（C 库，超低延迟）
- 片段长度: 0.5-1 秒
- 置信度阈值: 0.5

**输出**：
- 语音片段（带时间戳）
- VAD 置信度

---

### 3.3 目标说话人识别 (speaker_recognition)

**职责**：
- 注册目标好友声纹（15-30 秒录音）
- 实时提取语音片段的声纹嵌入
- 与注册库匹配，判断是否为目标好友
- 自适应阈值调整
- 持续学习更新声纹模型

**关键技术**：
- 声纹模型: ECAPA-TDNN / ResNet (pyannote.audio)
- 相似度计算: 余弦相似度
- 初始阈值: 0.75
- 特征维度: 192 或 512

**输出**：
- 说话人 ID（匹配成功）或 None（未匹配）
- 匹配置信度
- 门控决策（是否送 STT）

**注册流程**：
1. 用户录制目标好友 15-30 秒语音
2. 提取声纹嵌入向量
3. 保存到 `data/speaker_profiles/{friend_id}.npy`
4. 支持多次录音取平均提高鲁棒性

---

### 3.4 流式语音识别 (stt)

**职责**：
- 实时语音转文本
- 支持中英混说
- 输出时间戳对齐的文本
- 流式输出（边识别边推送）

**关键技术**：
- 本地方案: faster-whisper（CUDA 加速）
  - 模型: medium / small（平衡速度与准确率）
  - 语言: zh / auto
  - 批处理: 关闭（保证低延迟）
- 线上方案: 阿里云/腾讯云实时语音 API
  - 延迟更低（500ms-1s）
  - 按量计费

**输出**：
- 识别文本
- 时间戳（开始、结束）
- 说话人 ID
- 置信度

---

### 3.5 记忆与检索模块 (memory)

**职责**：
- 存储好友档案（姓名、偏好、禁忌话题、兴趣）
- 存储对话历史摘要
- 向量化文本并建立索引
- 支持语义检索（RAG）
- 定期更新与清理过时记忆

**关键技术**：
- 向量库: Chroma（轻量级，适合 demo）或 Faiss
- 文本嵌入: bge-m3 / text-embedding-ada-002
- 存储结构:
  ```json
  {
    "friend_id": "uuid",
    "name": "好友昵称",
    "preferences": ["喜欢动漫", "不喜欢政治话题"],
    "facts": [
      {"content": "最近在玩 XX 游戏", "timestamp": "2025-11-20", "embedding": [...]}
    ],
    "conversation_summaries": [...]
  }
  ```

**检索策略**：
- Top-K 检索（K=3-5）
- 时间衰减权重（近期记忆权重更高）
- 混合检索（关键词 + 语义）

**输出**：
- 相关记忆片段列表
- 相似度评分

---

### 3.6 会话理解与状态管理 (core)

**职责**：
- 提取对话事件（提问、陈述、请求、转场）
- 情绪/语气分析（可选）
- 维护对话状态机（话题、轮次）
- 判断是否需要生成建议

**关键技术**：
- 规则 + 小模型（intent classification）
- 状态机: 初始、倾听、回应、等待、转场
- 事件类型: QUESTION, STATEMENT, TOPIC_CHANGE, SILENCE

**输出**：
- 对话事件类型
- 当前状态
- 是否触发建议生成

---

### 3.7 LLM 推理层 (llm_client)

**职责**：
- 调用线上大语言模型 API（OpenAI / Claude / 国内模型）
- 构建 prompt（系统指令 + 对话上下文 + 检索记忆）
- 结构化输出（JSON schema / 函数调用）
- 工具函数支持（写入记忆、查询信息）

**Prompt 结构**：
```
【系统角色】
你是 VRChat 社交助手，帮助用户进行友好、得体的对话。

【当前上下文】
- 说话人: {friend_name}
- 最近发言: "{transcript}"
- 对话状态: {state}

【相关记忆】
- {friend_name} 喜欢动漫，最近在看《XX》
- 上次聊到了游戏话题

【安全边界】
- 不讨论政治、宗教
- 避免冒犯性言论

【任务】
生成 1-2 条社交建议，格式：
{
  "suggestions": [
    {"type": "direct_reply", "content": "你可以说：..."},
    {"type": "topic_guide", "content": "可以聊聊..."}
  ],
  "priority": "high",
  "notes": "对方在提问，建议快速回应"
}
```

**输出**：
- 结构化建议（JSON）
- 优先级
- 元数据（是否需要写入记忆）

---

### 3.8 提示生成策略 (prompt_generator)

**职责**：
- 格式化 LLM 输出为可读提示
- 分档位（超短提示 / 完整句 / 话题引导）
- 安全过滤（敏感词、隐私检查）
- 节流与优先级排序
- 提示过期管理

**档位设计**：
1. **即时提示**（1 行）："可以问：你最近在玩什么游戏？"
2. **完整建议**（2-3 行）："对方提到动漫，你可以说：'我也喜欢看动漫，你最近在追哪部？'"
3. **话题引导**："当前话题：游戏 → 建议转向：共同兴趣（动漫）"

**节流策略**：
- 5 秒内最多 1 条提示
- 高优先级可打断低优先级
- 过期提示（>10 秒）自动移除

**输出**：
- 格式化文本
- 显示时长
- 高亮关键字

---

### 3.9 VR 展示层 (vr_overlay)

**职责**：
- 使用 OpenXR 创建 Overlay 层
- 头显固定（head-locked）HUD 显示提示
- 世界锁定（world-locked）面板（可选）
- 交互控制（手柄按钮/手势开关）
- 可读性优化（字号、颜色、透明度）

**关键技术**：
- OpenXR API + Overlay 扩展
- 渲染: OpenGL / Vulkan
- 字体: 大字号（24-32pt）、高对比度
- 位置: 视野下方 15-20° 避免遮挡

**交互设计**：
- A 键: 切换显示/隐藏
- B 键: 切换档位（简洁/详细）
- 语音热词: "助手开/关"

**输出**：
- VR 空间中的文本 Overlay
- 实时更新

---

## 4. 数据模型

### 4.1 好友档案 (Friend Profile)
```json
{
  "id": "uuid",
  "name": "好友昵称",
  "voice_profile_path": "data/speaker_profiles/uuid.npy",
  "preferences": ["动漫", "游戏"],
  "avoid_topics": ["政治"],
  "personality": "外向/内向/随和",
  "last_seen": "2025-11-24T10:00:00Z",
  "conversation_count": 15
}
```

### 4.2 对话记录 (Conversation)
```json
{
  "id": "uuid",
  "friend_id": "uuid",
  "timestamp": "2025-11-24T10:05:30Z",
  "speaker_id": "friend_uuid",
  "transcript": "最近在玩 XX 游戏",
  "event_type": "STATEMENT",
  "embedding": [0.1, 0.2, ...],
  "confidence": 0.92
}
```

### 4.3 建议记录 (Suggestion)
```json
{
  "id": "uuid",
  "conversation_id": "uuid",
  "type": "direct_reply",
  "content": "你可以说：我也在玩！",
  "priority": "high",
  "created_at": "2025-11-24T10:05:32Z",
  "displayed": true,
  "adopted": false
}
```

---

## 5. 技术选型

### 5.1 核心依赖

| 模块 | 技术方案 | 备选方案 |
|------|---------|----------|
| 音频采集 | sounddevice + WASAPI | PyAudio |
| VAD | Silero VAD | WebRTC VAD |
| 说话人识别 | pyannote.audio (ECAPA-TDNN) | SpeechBrain |
| STT | faster-whisper | 阿里云/讯飞实时 ASR |
| 文本嵌入 | sentence-transformers (bge-m3) | OpenAI Embeddings |
| 向量库 | Chroma | Faiss / Milvus |
| LLM | OpenAI API / Claude API | 本地 Llama / Qwen |
| VR Overlay | OpenXR + pyopenvr | OVR Toolkit |
| 前端框架 | Python asyncio | - |

### 5.2 开发环境
- Python 3.10+
- CUDA 11.8+（GPU 加速 STT/声纹）
- SteamVR / Oculus Runtime
- Windows 10/11

---

## 6. 性能指标

### 6.1 延迟目标

| 阶段 | 目标延迟 | 最大可接受 |
|------|---------|----------|
| VAD 检测 | 30ms | 50ms |
| 说话人识别 | 50ms | 100ms |
| STT 识别 | 500ms | 1000ms |
| LLM 推理 | 800ms | 1500ms |
| 提示生成 | 20ms | 50ms |
| VR 渲染 | 16ms (60fps) | 33ms (30fps) |
| **端到端** | **1.4s** | **2.7s** |

### 6.2 准确率目标
- VAD 准确率: > 95%
- 说话人识别: > 85%（1-3 个注册好友）
- STT 准确率: > 90%（中英混说）
- 建议相关性: 主观评估 > 80%

### 6.3 资源占用
- CPU: < 30%（12 线程）
- GPU: < 2GB VRAM
- 内存: < 4GB
- 磁盘: < 500MB（不含对话历史）

---

## 7. 安全与合规

### 7.1 隐私保护
- **本地化处理**：语音数据不上传云端（除非使用线上 STT）
- **匿名化**：对话记录只保存文本摘要，不保存原始音频
- **加密存储**：向量库与好友档案可选 AES-256 加密
- **用户控制**：支持一键删除所有记忆数据

### 7.2 合规性
- **非注入设计**：使用系统级 OpenXR Overlay，不修改 VRChat 客户端
- **只读展示**：不自动发送消息或操作角色
- **透明化**：明确告知用户系统功能与数据使用

### 7.3 安全边界
- **敏感词过滤**：政治、宗教、歧视性内容
- **隐私保护**：不泄露他人隐私信息
- **误导防范**：避免生成虚假信息或有害建议

---

## 8. 开发路线图

### Phase 1: MVP Demo（2-3 周）

**Week 1: 音频管道**
- [ ] 音频采集模块（系统回环 + 麦克风）
- [ ] VAD 集成（Silero）
- [ ] 说话人注册与识别（ECAPA-TDNN）
- [ ] 流式 STT（faster-whisper）
- [ ] 桌面窗口打印识别结果（不进 VR）

**Week 2: 智能层**
- [ ] 向量库搭建（Chroma）
- [ ] 好友档案管理（CRUD）
- [ ] RAG 检索实现
- [ ] LLM 客户端（OpenAI API）
- [ ] 提示生成策略（简单版）
- [ ] 桌面 GUI 展示建议

**Week 3: VR 集成**
- [ ] OpenXR Overlay 基础框架
- [ ] 头显 HUD 文本渲染
- [ ] 手柄交互（显示/隐藏）
- [ ] 端到端集成测试
- [ ] 延迟优化

### Phase 2: 功能增强（1-2 个月）
- [ ] 会话状态管理
- [ ] 情绪分析
- [ ] 多档位建议
- [ ] 安全词与边界
- [ ] 自适应阈值调整
- [ ] 持续学习声纹更新
- [ ] 配置界面
- [ ] 日志与监控

### Phase 3: 优化与扩展（持续）
- [ ] 多人场景处理（2-5 人同时说话）
- [ ] 方向性音频权重
- [ ] 本地 LLM 支持（离线模式）
- [ ] 高级交互（手势控制）
- [ ] 云同步（可选）
- [ ] 社区反馈迭代

---

## 9. 风险与挑战

### 9.1 技术风险

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| 环境噪声影响识别 | 高 | 增强降噪、自适应阈值 |
| 多人重叠讲话 | 中 | 优先处理最近声源 |
| GPU 资源不足 | 中 | 提供 CPU 降级方案 |
| 网络延迟（线上 API） | 中 | 本地模型备份 |
| OpenXR 兼容性 | 低 | 支持主流头显（Quest/Index/Vive） |

### 9.2 体验风险
- **提示刷屏**：采用节流与优先级排序
- **建议不相关**：持续优化 prompt 与 RAG
- **延迟过高**：流式化所有环节，优化模型尺寸
- **隐私担忧**：透明化数据使用，提供删除功能

### 9.3 合规风险
- **VRChat ToS**：确保不注入客户端，仅做系统 Overlay
- **语音识别准确性**：添加置信度提示，避免误导

---

## 10. 未来扩展方向

1. **多语言支持**：日语、韩语等 VRChat 常用语言
2. **情绪共鸣**：根据对方情绪调整建议语气
3. **话题知识库**：预置常见 VRChat 话题（游戏、活动、技术）
4. **社交技能训练**：回顾对话，提供改进建议
5. **群组场景优化**：多人对话的摘要与参与度分析
6. **可穿戴设备集成**：手表、手环等辅助输入
7. **开放插件系统**：允许社区扩展功能模块

---

## 附录

### A. 参考资料
- [OpenXR Specification](https://www.khronos.org/openxr/)
- [Faster Whisper](https://github.com/guillaumekln/faster-whisper)
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio)
- [Chroma Vector Database](https://www.trychroma.com/)
- [Silero VAD](https://github.com/snakers4/silero-vad)

### B. 词汇表
- **VAD**: Voice Activity Detection（语音活动检测）
- **STT**: Speech-to-Text（语音转文本）
- **RAG**: Retrieval-Augmented Generation（检索增强生成）
- **Overlay**: VR 中的叠加层界面
- **Head-Locked**: 头显固定（跟随头部移动）
- **World-Locked**: 世界锁定（固定在 VR 空间位置）

---

**文档版本**: v1.0  
**最后更新**: 2025-11-24  
**维护者**: 开发团队